
@misc{word2vec,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781} 
}

@misc{simCSE,
      title={SimCSE: Simple Contrastive Learning of Sentence Embeddings}, 
      author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
      year={2022},
      eprint={2104.08821},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08821}, 
}

@inproceedings{GloVe,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@misc{bojanowski2017enrichingwordvectorssubword,
      title={Enriching Word Vectors with Subword Information}, 
      author={Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
      year={2017},
      eprint={1607.04606},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1607.04606}, 
}


@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@misc{GPT4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{INSTRUCTOR,
      title={One Embedder, Any Task: Instruction-Finetuned Text Embeddings}, 
      author={Hongjin Su and Weijia Shi and Jungo Kasai and Yizhong Wang and Yushi Hu and Mari Ostendorf and Wen-tau Yih and Noah A. Smith and Luke Zettlemoyer and Tao Yu},
      year={2023},
      eprint={2212.09741},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09741}, 
}

@misc{task_retrieval,
      title={Task-aware Retrieval with Instructions}, 
      author={Akari Asai and Timo Schick and Patrick Lewis and Xilun Chen and Gautier Izacard and Sebastian Riedel and Hannaneh Hajishirzi and Wen-tau Yih},
      year={2022},
      eprint={2211.09260},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.09260}, 
}


@misc{MTEB,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07316}, 
}


@misc{BEIR,
      title={BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models}, 
      author={Nandan Thakur and Nils Reimers and Andreas Rücklé and Abhishek Srivastava and Iryna Gurevych},
      year={2021},
      eprint={2104.08663},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2104.08663}, 
}

@article{MKQA,
    title = "{MKQA}: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
    author = "Longpre, Shayne  and
      Lu, Yi  and
      Daiber, Joachim",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.82",
    doi = "10.1162/tacl_a_00433",
    pages = "1389--1406",
    abstract = "Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages.1",
}
/MLDR Multilingual Long-Document Retrieval dataset 
@misc{bge-m3,
      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, 
      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2402.03216},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{long-embed,
      title={LongEmbed: Extending Embedding Models for Long Context Retrieval}, 
      author={Dawei Zhu and Liang Wang and Nan Yang and Yifan Song and Wenhao Wu and Furu Wei and Sujian Li},
      year={2024},
      eprint={2404.12096},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.12096}, 
}



@misc{E5mistral,
      title={Improving Text Embeddings with Large Language Models}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Linjun Yang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2401.00368},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.00368}, 
}
@misc{dpr,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.04906}, 
}

@misc{e5embed,
      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2212.03533},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.03533}, 
}


@misc{T5,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misc{nv-embed,
      title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models}, 
      author={Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},
      year={2024},
      eprint={2405.17428},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17428}, 
}

@misc{UnilR,
      title={UniIR: Training and Benchmarking Universal Multimodal Information Retrievers}, 
      author={Cong Wei and Yang Chen and Haonan Chen and Hexiang Hu and Ge Zhang and Jie Fu and Alan Ritter and Wenhu Chen},
      year={2023},
      eprint={2311.17136},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.17136}, 
}

@misc{CLIP,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@misc{BLIP,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      eprint={2201.12086},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.12086}, 
}

@misc{VLM2Vec,
      title={VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks}, 
      author={Ziyan Jiang and Rui Meng and Xinyi Yang and Semih Yavuz and Yingbo Zhou and Wenhu Chen},
      year={2024},
      eprint={2410.05160},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.05160}, 
}

@misc{MM-GEM,
      title={Multi-Modal Generative Embedding Model}, 
      author={Feipeng Ma and Hongwei Xue and Guangting Wang and Yizhou Zhou and Fengyun Rao and Shilin Yan and Yueyi Zhang and Siying Wu and Mike Zheng Shou and Xiaoyan Sun},
      year={2024},
      eprint={2405.19333},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.19333}, 
}


@misc{GMC,
      title={Geometric Multimodal Contrastive Representation Learning}, 
      author={Petra Poklukar and Miguel Vasco and Hang Yin and Francisco S. Melo and Ana Paiva and Danica Kragic},
      year={2022},
      eprint={2202.03390},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.03390}, 
}

@misc{retromae,
      title={RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder}, 
      author={Shitao Xiao and Zheng Liu and Yingxia Shao and Zhao Cao},
      year={2022},
      eprint={2205.12035},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.12035}, 
}


@misc{bge-cpack,
      title={C-Pack: Packed Resources For General Chinese Embeddings}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},
      year={2024},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.07597}, 
}


@misc{simLM,
      title={SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
      year={2023},
      eprint={2207.02578},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2207.02578}, 
}

@misc{ANCE,
      title={Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval}, 
      author={Lee Xiong and Chenyan Xiong and Ye Li and Kwok-Fung Tang and Jialin Liu and Paul Bennett and Junaid Ahmed and Arnold Overwijk},
      year={2020},
      eprint={2007.00808},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2007.00808}, 
}


@article{Narrativeqa,
    title = "The {N}arrative{QA} Reading Comprehension Challenge",
    author = "Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}}  and
      Schwarz, Jonathan  and
      Blunsom, Phil  and
      Dyer, Chris  and
      Hermann, Karl Moritz  and
      Melis, G{\'a}bor  and
      Grefenstette, Edward",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1023",
    doi = "10.1162/tacl_a_00023",
    pages = "317--328",
    abstract = "Reading comprehension (RC){---}in contrast to information retrieval{---}requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.",
}


@misc{hyde,
      title={Precise Zero-Shot Dense Retrieval without Relevance Labels}, 
      author={Luyu Gao and Xueguang Ma and Jimmy Lin and Jamie Callan},
      year={2022},
      eprint={2212.10496},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2212.10496}, 
}

@misc{query2doc,
      title={Query2doc: Query Expansion with Large Language Models}, 
      author={Liang Wang and Nan Yang and Furu Wei},
      year={2023},
      eprint={2303.07678},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2303.07678}, 
}

@misc{stepback,
      title={Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models}, 
      author={Huaixiu Steven Zheng and Swaroop Mishra and Xinyun Chen and Heng-Tze Cheng and Ed H. Chi and Quoc V Le and Denny Zhou},
      year={2024},
      eprint={2310.06117},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.06117}, 
}

@misc{Focal-info,
      title={Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE}, 
      author={Pengyue Hou and Xingyu Li},
      year={2023},
      eprint={2310.06918},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06918}, 
}
@article{gte,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}


@article{Tatoeba,
   title={Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond},
   volume={7},
   ISSN={2307-387X},
   url={http://dx.doi.org/10.1162/tacl_a_00288},
   DOI={10.1162/tacl_a_00288},
   journal={Transactions of the Association for Computational Linguistics},
   publisher={MIT Press - Journals},
   author={Artetxe, Mikel and Schwenk, Holger},
   year={2019},
   month=nov, pages={597–610} 
}

@misc{LORA,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@article{bajaj2016ms,
  title={Ms marco: A human generated machine reading comprehension dataset},
  author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal={arXiv preprint arXiv:1611.09268},
  year={2016}
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@article{lewis2021paq,
  title={PAQ: 65 million probably-asked questions and what you can do with them},
  author={Lewis, Patrick and Wu, Yuxiang and Liu, Linqing and Minervini, Pasquale and K{\"u}ttler, Heinrich and Piktus, Aleksandra and Stenetorp, Pontus and Riedel, Sebastian},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1098--1115},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@misc{stack-exchage,
  title={Stack Exchange Data Dump},
  author={Stack-Exchange-Community},
  year={2023},
}


@misc{stanford2022stanford,
  title={The Stanford Natural Language Inference (SNLI) Corpus},
  author={Stanford NLP Group and others},
  year={2022}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@inproceedings{wachsmuth2018retrieval,
  title={Retrieval of the best counterargument without prior topic knowledge},
  author={Wachsmuth, Henning and Syed, Shahbaz and Stein, Benno},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={241--251},
  year={2018}
}

@article{tsatsaronis2015overview,
  title={An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition},
  author={Tsatsaronis, George and Balikas, Georgios and Malakasiotis, Prodromos and Partalas, Ioannis and Zschunke, Matthias and Alvers, Michael R and Weissenborn, Dirk and Krithara, Anastasia and Petridis, Sergios and Polychronopoulos, Dimitris and others},
  journal={BMC bioinformatics},
  volume={16},
  pages={1--28},
  year={2015},
  publisher={Springer}
}

@inproceedings{maia201818,
  title={Www'18 open challenge: financial opinion mining and question answering},
  author={Maia, Macedo and Handschuh, Siegfried and Freitas, Andr{\'e} and Davis, Brian and McDermott, Ross and Zarrouk, Manel and Balahur, Alexandra},
  booktitle={Companion proceedings of the the web conference 2018},
  pages={1941--1942},
  year={2018}
}

@article{thorne2018fever,
  title={FEVER: a large-scale dataset for fact extraction and VERification},
  author={Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  journal={arXiv preprint arXiv:1803.05355},
  year={2018}
}

@article{me5,
  title={Multilingual E5 Text Embeddings: A Technical Report},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2402.05672},
  year={2024}
}


@article{muennighoff2022mteb,
  title={{MTEB}: Massive text embedding benchmark},
  author={Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  journal={arXiv preprint arXiv:2210.07316},
  year={2022}
}


@inproceedings{mcauley-2013-hiddenfactors,
author = {McAuley, Julian and Leskovec, Jure},
title = {Hidden factors and hidden topics: understanding rating dimensions with review text},
year = {2013},
isbn = {9781450324090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2507157.2507163},
doi = {10.1145/2507157.2507163},
abstract = {In order to recommend products to users we must ultimately predict how a user will respond to a new product. To do so we must uncover the implicit tastes of each user as well as the properties of each product. For example, in order to predict whether a user will enjoy Harry Potter, it helps to identify that the book is about wizards, as well as the user's level of interest in wizardry. User feedback is required to discover these latent product and user dimensions. Such feedback often comes in the form of a numeric rating accompanied by review text. However, traditional methods often discard review text, which makes user and product latent dimensions difficult to interpret, since they ignore the very text that justifies a user's rating. In this paper, we aim to combine latent rating dimensions (such as those of latent-factor recommender systems) with latent review topics (such as those learned by topic models like LDA). Our approach has several advantages. Firstly, we obtain highly interpretable textual labels for latent rating dimensions, which helps us to `justify' ratings with text. Secondly, our approach more accurately predicts product ratings by harnessing the information present in review text; this is especially true for new products and users, who may have too few ratings to model their latent factors, yet may still provide substantial information from the text of even a single review. Thirdly, our discovered topics can be used to facilitate other tasks such as automated genre discovery, and to identify useful and representative reviews.},
booktitle = {Proceedings of the 7th ACM Conference on Recommender Systems},
pages = {165–172},
numpages = {8},
keywords = {recommender systems, topic models},
location = {Hong Kong, China},
series = {RecSys '13}
}


@article{o2021wish,
  title={I Wish I Would Have Loved This One, But I Didn't--A Multilingual Dataset for Counterfactual Detection in Product Reviews},
  author={O'Neill, James and Rozenshtein, Polina and Kiryo, Ryuichi and Kubota, Motoko and Bollegala, Danushka},
  journal={arXiv preprint arXiv:2104.06893},
  year={2021}
}


@inproceedings{Casanueva2020,
    author      = {I{\~{n}}igo Casanueva and Tadas Temcinas and Daniela Gerz and Matthew Henderson and Ivan Vulic},
    title       = {Efficient Intent Detection with Dual Sentence Encoders},
    year        = {2020},
    month       = {mar},
    note        = {Data available at https://github.com/PolyAI-LDN/task-specific-datasets},
    url         = {https://arxiv.org/abs/2003.04807},
    booktitle   = {Proceedings of the 2nd Workshop on NLP for ConvAI - ACL 2020}
}

@inproceedings{saravia-etal-2018-carer,
    title = "{CARER}: Contextualized Affect Representations for Emotion Recognition",
    author = "Saravia, Elvis  and
      Liu, Hsien-Chi Toby  and
      Huang, Yen-Hao  and
      Wu, Junlin  and
      Chen, Yi-Shin",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1404",
    doi = "10.18653/v1/D18-1404",
    pages = "3687--3697",
    abstract = "Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand emotion, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. We propose a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text. The pattern-based representations are further enriched with word embeddings and evaluated through several emotion recognition tasks. Our experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on emotion recognition tasks.",
}


@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@inproceedings{li-etal-2021-mtop,
    title = "{MTOP}: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark",
    author = "Li, Haoran  and
      Arora, Abhinav  and
      Chen, Shuohui  and
      Gupta, Anchit  and
      Gupta, Sonal  and
      Mehdad, Yashar",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.257",
    doi = "10.18653/v1/2021.eacl-main.257",
    pages = "2950--2962",
    abstract = "Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called MTOP, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pre-trained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pre-trained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.",
}

@misc{kaggle2019jigsaw,
    author = {C.J. Adams and Daniel Borkan and Jeffrey Sorensen and Lucas Dixon and Lucy Vasserman and Nithum Thain},
    title = {Jigsaw Unintended Bias in Toxicity Classification},
    publisher = {Kaggle},
    year = {2019},
    url = {https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification}
}



@misc{tweet-sentiment-extraction,
    author = {Maggie, Phil Culliton, Wei Chen},
    title = {Tweet Sentiment Extraction},
    publisher = {Kaggle},
    year = {2020},
    url = {https://kaggle.com/competitions/tweet-sentiment-extraction}
}

@incollection{lang1995newsweeder,
  title={Newsweeder: Learning to filter netnews},
  author={Lang, Ken},
  booktitle={Machine learning proceedings 1995},
  pages={331--339},
  year={1995},
  publisher={Elsevier}
}


@inproceedings{agirre-etal-2012-semeval,
    title = "{S}em{E}val-2012 Task 6: A Pilot on Semantic Textual Similarity",
    author = "Agirre, Eneko  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor",
    editor = "Agirre, Eneko  and
      Bos, Johan  and
      Diab, Mona  and
      Manandhar, Suresh  and
      Marton, Yuval  and
      Yuret, Deniz",
    booktitle = "*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    month = "7-8 " # jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S12-1051",
    pages = "385--393",
}

@inproceedings{chen-etal-2022-semeval,
    title = "{S}em{E}val-2022 Task 8: Multilingual news article similarity",
    author = {Chen, Xi  and
      Zeynali, Ali  and
      Camargo, Chico  and
      Fl{\"o}ck, Fabian  and
      Gaffney, Devin  and
      Grabowicz, Przemyslaw  and
      Hale, Scott  and
      Jurgens, David  and
      Samory, Mattia},
    editor = "Emerson, Guy  and
      Schluter, Natalie  and
      Stanovsky, Gabriel  and
      Kumar, Ritesh  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      Singh, Siddharth  and
      Ratan, Shyam",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.semeval-1.155",
    doi = "10.18653/v1/2022.semeval-1.155",
    pages = "1094--1106",
    abstract = "Thousands of new news articles appear daily in outlets in different languages. Understanding which articles refer to the same story can not only improve applications like news aggregation but enable cross-linguistic analysis of media consumption and attention. However, assessing the similarity of stories in news articles is challenging due to the different dimensions in which a story might vary, e.g., two articles may have substantial textual overlap but describe similar events that happened years apart. To address this challenge, we introduce a new dataset of nearly 10,000 news article pairs spanning 18 language combinations annotated for seven dimensions of similarity as SemEval 2022 Task 8. Here, we present an overview of the task, the best performing submissions, and the frontiers and challenges for measuring multilingual news article similarity. While the participants of this SemEval task contributed very strong models, achieving up to 0.818 correlation with gold standard labels across languages, human annotators are capable of reaching higher correlations, suggesting space for further progress.",
}

@inproceedings{cer-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2001",
    doi = "10.18653/v1/S17-2001",
    pages = "1--14",
    abstract = "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in \textit{all language tracks}. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the \textit{STS Benchmark} is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).",
}

@misc{lee2024geckoversatiletextembeddings,
      title={Gecko: Versatile Text Embeddings Distilled from Large Language Models}, 
      author={Jinhyuk Lee and Zhuyun Dai and Xiaoqi Ren and Blair Chen and Daniel Cer and Jeremy R. Cole and Kai Hui and Michael Boratko and Rajvi Kapadia and Wen Ding and Yi Luan and Sai Meher Karthik Duddu and Gustavo Hernandez Abrego and Weiqiang Shi and Nithi Gupta and Aditya Kusupati and Prateek Jain and Siddhartha Reddy Jonnalagadda and Ming-Wei Chang and Iftekhar Naim},
      year={2024},
      eprint={2403.20327},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.20327}, 
}

@misc{SFRAIResearch2024,
  title={SFR-Embedding-Mistral:Enhance Text Retrieval with Transfer Learning},
  author={Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz},
  howpublished={Salesforce AI Research Blog},
  year={2024},
  url={https://blog.salesforceairesearch.com/sfr-embedded-mistral/}
}
