\subsection{Model Architecture}
I adopt Low-Rank Adaptation (LoRA)\cite{LORA}, a parameter-efficient fine-tuning (PEFT) method, to effectively finetune the our model.
I employ Mistral-7B \cite{mistral7b}, a decoder-only LLM, as the foundational architecture. The embeddings is obtained by appending the [EOS] token to end of sequence and taking the [EOS] vector. I adopt Low-Rank Adaptation (LoRA) , a parameter-efficient fine-tuning (PEFT) method, to effectively finetune the our model. I adopt the Focal-InfoNCE loss \cite{Focal-info} $\mathbb{L}$ over the in-batch negatives and hard negatives. Within a mini-batch of N, given relevant query-document pair ($q^+$, $d^+$), the loss function is:
\begin{equation} \label{equ:focalinfonce}
\min\ \ \mathbb{L}= -log\frac{e^{\sum_{i=1}^N(\phi(q^+, d^+))^2/\tau}}
{\Pi_{i=1}^{N}\left[\displaystyle\sum_{n_i \in \mathbb{N}} e^{\phi(q^+, n_i)(\phi(q^+, n_i) + m)/\tau} + e^{(\phi(q^+, d^+))^2/\tau}\right]}
\end{equation}

where $\tau$ is a temperature hyperparameter, m is a hardness-aware hyperparameter that offers flexibility in adjusting the re-weighting strategy, $\mathbb{N}$ denotes the set of all negatives and  $\phi(q, d)$ denotes the cosine similarity score between embeddings of a query $q$ and document $d$. 

\subsection{Training Data}
To evaluate the proposed methodologies, I will employ a set of public datasets by adapting the setup from NV-Embed\cite{nv-embed} to demonstrate the versatility of the proposed approaches in various embedding tasks, encompassing both retrieval and non-retrieval benchmarks. For retrieval tasks, I will utilize datasets including MS MARCO~\cite{bajaj2016ms}, HotpotQA~\cite{yang2018hotpotqa}, Natural Questions~\cite{kwiatkowski2019natural}, PAQ~\cite{lewis2021paq}, StackExchange~\cite{stack-exchage}, SQuAD~\cite{rajpurkar2016squad}, ArguAna~\cite{wachsmuth2018retrieval}, BioASQ~\cite{tsatsaronis2015overview}, FiQA~\cite{maia201818}, and FEVER~\cite{thorne2018fever}. As datasets without its hard negatives, these will be obtained by utilizing $mE5_{base}$ \cite{me5} to mine the top 100 hard negatives. For non-retrieval tasks, I will utilize the training splits of datasets from three sub-tasks in the MTEB benchmark: classification, clustering, and semantic textual similarity (STS) )â€”sourced from the MTEB Hugging Face datasets~\cite{muennighoff2022mteb}. The included datasets will be: AmazonReviews-Classification~\cite{mcauley-2013-hiddenfactors}, Amazon-Counterfactual-Classification~\cite{o2021wish}, Banking77-Classification~\cite{Casanueva2020}, Emotion-Classification~\cite{saravia-etal-2018-carer}, IMDB-Classification~\cite{maas-EtAl:2011:ACL-HLT2011}, MTOPIntent-Classification~\cite{li-etal-2021-mtop}, ToxicConversations-Classification~\cite{kaggle2019jigsaw}, TweetSentimentExtraction-Classification~\cite{tweet-sentiment-extraction}, clustering datasets such as TwentyNewsgroups~\cite{lang1995newsweeder}, raw\_arxiv, raw\_biorxiv, and raw\_medrxiv, and semantic similarity datasets such as STS12~\cite{agirre-etal-2012-semeval}, STS22~\cite{chen-etal-2022-semeval}, and the STS-Benchmark~\cite{cer-etal-2017-semeval}. To ensure meaningful evaluation, common content between raw\_arxiv, raw\_biorxiv, raw\_medrxiv, and TwentyNewsgroups-Clustering datasets will be filtered against the MTEB evaluation set to prevent overlap. I will only utilize the training splits from each dataset to maintain consistent comparison and benchmarking throughout this study.
\\
As introduced in Section~\ref{sec:Methodology}, for each query-document pair ($q$, $d$) in datasets associated with retrieval tasks, training pairs will be generated based on the proposed methodology. These include ($q'$, $d$), ($q'$, $d_H$), and ($q'{abs}$, $d$), where $q'$ represents an instruction-based query, $d_H$ denotes to synthetic hypothetical document of $d$, and $q'{abs}$ incorporates task abstraction. GPT-4 will be utilized to generate the hypothetical documents and task abstractions.
% \subsection{Model Fine-tuning Dataset}


% \subsection{Evaluation Metrics and Dataset and model}
\subsection{Evaluation}
To comprehensively evaluate the proposed methodologies, the models will be assessed on the Massive Text Embedding Benchmark (MTEB), which encompasses 15 retrieval datasets, 4 reranking datasets, 12 classification datasets, 11 clustering datasets, 3 pair classification datasets, 10 semantic textual similarity datasets, and 1 summarization dataset, providing a holistic view of performance across diverse use cases. Additionally, the models' capabilities will be tested on the BEIR benchmark, a heterogeneous benchmark containing diverse information retrieval tasks. I will compare our model with recent frontier embedding models, including e5-mistral-7b-instruct\cite{E5mistral}, Google Gecko\cite{lee2024geckoversatiletextembeddings}, SFR-Embedding\cite{SFRAIResearch2024}, NV-Embed\cite{nv-embed}, gte-Qwen2-7B-instruct\cite{gte}, to benchmark both retrieval accuracy and downstream task performance. The evaluation aims to validate the effectiveness of synthetic hypothetical document training and task abstraction methods in advancing retrieval-augmented generation embedding models.

\subsection{Ablation Studies}
To rigorously assess the contributions of each proposed component, I will conduct a series of ablation studies.

First, I will isolate the impact of the Synthetic Hypothetical Document Training by conducting experiments with and without augmenting training data with synthetic hypothetical documents generated by LLMs, measuring their influence on embedding model.

Second, to examine the impact of  \textit{Task Abstraction for Instruction-Based Fine-Tuning}, I will train models with and without task abstraction instructions and comparing their task generalization capabilities.This will reveal how task abstraction enhances embedding generalization and semantic alignment. 

These ablation studies aim to provide deeper insights into the effectiveness and limitations of the proposed methods, validating their individual and combined impacts on enhancing embedding models.
