Recent advancements in Retrieval Augmented Generation (RAG) heavily depend on embedding models for effective retrieval and reasoning, yet these models face critical challenges in real-world applications. Current embeddings often fail to generalize well to complex tasks such as multi-hop reasoning, which require understanding and connecting semantic relationships across multiple documents. To address these limitations, this research proposes two methods: (1) \textbf{Synthetic Hypothetical Document Training}, which leverages large language models (LLMs) to generate synthetic hypothetical documents that enrich embedding training and improve their capacity to capture semantic relationships, and (2) \textbf{Task Abstraction for Instruction-Based Finetuning}, which introduces task-specific, high-level instructions as auxiliary input during model training to align embeddings with task semantics and enhance their generalizability. Together, these approaches aim to advance the state of embedding models in RAG by improving both retrieval accuracy and semantic understanding while enabling better performance in multi-hop reasoning and task-specific scenarios.


% Recent advancements in Retrieval Augmented Generation (RAG) rely heavily on high-quality embedding models for accurate retrieval and reasoning. However, existing embeddings still have significant limitations in real-world applications and often fail to generalize well to complex tasks such as multi-hop reasoning because they cannot capture fine-grained semantic relationships across documents. To address these challenges, this research investigates: (1) Hypothetical Document Embedding), a novel synthetic data augmentation approach leveraging large language models, and (2) a task abstraction mechanism for instruction-based model fine-tuning. These methods aim to significantly improve embedding modelsâ€™ capacity for generalization and semantic alignment.