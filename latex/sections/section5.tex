This literature review highlights the pivotal role of embedding models in Retrieval-Augmented Generation (RAG) systems, focusing on encoder-only, encoder-decoder, decoder-only, and multi-modal embeddings. We explored training paradigms such as pretraining, contrastive learning, and task-specific fine-tuning, along with benchmarks like MTEB to assess embedding performance. While significant advances have been made, challenges remain in retrieval relevance, factual grounding, and multi-modal integration. Future work should address these limitations to enhance the effectiveness and scalability of embedding models in RAG pipelines.